---
title: "Test Continental DAISIE"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Test Continental DAISIE}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(DAISIEcontinental)
```

The {DAISIE} R package contains maximum likelihood models to infer island biogeographical and macroevolutionary processes on reconstructed phylogenetic data of island communities. A new model in the DAISIE model arsenal is the continental island model. This model adjusts the initial conditions of the DAISIE model and introduces a new parameter (`p`), the probability of initial presence. In other words, the parameter defines what proportion of the island community derives from a vicariance event at the origin of the island compared to colonising the island after it formed. 

The {DAISIEcontinental} R package is a set of tools (functions and scripts) to establish and quantify how accurate the continental DAISIE model is for a variety of island scenarios (e.g. different island ages, and community size).

The package has three principal analyses:

* continental inference model estimation performance with simulated data
* profile likelihood of empirical data at fixed values of `p`
* estimation accuracy of vicariance under idealised simulations varying inference model settings 

The `run_empirical_analysis` (bash and R script) can be used on empirical data when we do not know the true value of `p`, and want to understand the profile of the likelihood. The `run_empirical_analysis` script is used when empirical data is available and we want to determine how the maximised likelihood of the model changes as we fix the probability of initial presence to different values between its bounds (`[0, 1]`). The `run_analysis` (bash and R script) is to determine the precision of the model with a known value of `p`. The `run_analysis` script is used when we want to determine how well the continental model can estimate the probability of initial presence for a given simulated scenario. The `run_vicariance_analysis` (bash and R script) is used to determine how well the maximum likelihood model can estimate the probability of initial presence across different island ages and inference settings when `p = 1` and there is no extinction or colonisation.

This vignette will walk through the application of all of these analyses and show how they can be used to analyse the continental DAISIE model. The package leverages the High Performance Cluster Computer (HPCC) from the University of Groningen to run the analyses in a distributed manner to parallelise computationally demanding analyses.

## Reproducing parameter estimation analysis

### Running the analysis on the HPC

* Clone the repository
* Change the working directory into `/DAISIEcontinental`
* Install the repository by running `sbatch bash/install_DAISIEcontinental.sh`
* Once the installation has finished, run the analysis this requires multiple
steps.

The analysis cannot be submitted using a single command. This is because the 
analysis is run over a parameter space, with each parameter set being run
for several replicates. Each replicate is a single job submitted by job array
(see `bash/run_analysis.sh`).

Each parameter set in the parameter space cannot be submitted by a single 
bash script as it would require two job arrays, one going over the parameter
sets and a second over the replicates. However, this case (which could be set
up by having a master bash script which arrays over the parameter space which
submits multiple lower-level bash scripts each submitting multiple replicates)
does not work in practise as it would write the logs of multiple jobs to the 
same log file. This cannot be worked around as the `#!/bin/bash` bash header
does not accept arguments to name the output log files.

Therefore, what works is to manually submit each parameter set manually by
specifying the parameter set index with an argument to the bash script. There
are 21 parameter sets so the entire parameter space can be run by running the 
following commands (still with `DAISIEcontinental` as the working directory):

* `sbatch bash/run_analysis.sh 1`
* `sbatch bash/run_analysis.sh 2`
* `sbatch bash/run_analysis.sh 3`
* ...
* `sbatch bash/run_analysis.sh 19`
* `sbatch bash/run_analysis.sh 20`
* `sbatch bash/run_analysis.sh 21`

This results in each log and results file having a unique file name and each
job writing to a single file.

The reasoning for having the the jobs so distributed, i.e. each job only runs
a single replicate for three different island ages, is because running more
simulations and model fitting sequentially does not scale well for the number
of replicates. To ensure an adequate sample size of results the model should be
run for each island age and each parameter set several hundred times, to account
for model stochasticity. Simulations that have a large community size (number of
species) take several minutes to run each. If these were to run sequentially
for, say 500 replicates, each with three island ages, totalling 1500 replicates
it would take an exceedingly long time to compute. The setup implemented
reduces this run time by running replicates over separate cores on a cluster.

The results can then be copied to a directory in `\inst` called 
`pre_processed_daisie_results`. If this folder is not already present it can be
created by running `mkdir inst/pre_processed_daisie_results`. The results can
be copied by running `cp -r results/ inst/pre_processed_daisie_results`. These
results can then be pushed to the repository.

### Processing DAISIE results

Before plotting the results, they need to be collated and formatted. This is
done by using the `post_process_daisie_results()` function. The 
`post_process.R` script in `scripts/` can be run either by
opening R and running the code (either copying and pasting into the console or
calling with `source()`), or can be run from the terminal with 
`Rscript scripts/post_process.R` (note in both cases the 
DAISIEcontinental package will need to be load to use the function). This 
function will automatically save the post-processed files in 
`inst/post_processed_daisie_results/`.

The pre-processed results from the cluster jobs can be deleted as there are a 
large number of files and there is no need to have them tracked by version 
control. Therefore, the `inst/pre_processed_daisie_results` directory can be
deleted with `rm -r inst/pre_processed_daisie_results`.

## Reproducing profile likelihood empirical analysis

The empirical analysis uses the same startup steps as the parameter estimation
analysis. If these steps have already been done for the above analysis they do
not need to be repeated.

* Clone the repository
* Change the working directory into `/DAISIEcontinental`
* Install the repository by running `sbatch bash/install_DAISIEcontinental.sh`

The empirical analysis uses the same job setup and submission as the parameter
estimation analysis. The entire empirical analysis also cannot be submitted 
with a single command, and instead each parameter set is submitted individually, 
specifying the param set index as an argument. There are 50 parameter sets,
which includes each taxonomic group and each fixed probability of initial 
presence of species on the continental island.

* `sbatch bash/run_empirical_analysis.sh 1`
* `sbatch bash/run_empirical_analysis.sh 2`
* `sbatch bash/run_empirical_analysis.sh 3`
* ...
* `sbatch bash/run_empirical_analysis.sh 48`
* `sbatch bash/run_empirical_analysis.sh 49`
* `sbatch bash/run_empirical_analysis.sh 50`

### Processing empirical results

The empirical results are also post-processed to collate the results into each
parameter set. The second function in the `scripts/post_process.R`

```r
post_process_empirical_analysis(
  data_folder_path = file.path("inst", "pre_processed_empirical_results"),
  output_file_path = file.path("inst", "post_processed_empirical_results")
)
```

will create the post-processed results and put them in the
`inst/post_processed_empirical_results` directory. From there they can be
committed to version control, and used to make plots.

## Vicariance analysis

The vicariance analysis uses the same startup steps of cloning, installation,
and using the correct working directory as the parameter estimation
analysis. If these steps have already been done for the above analysis they do
not need to be repeated.

The parameter space is created within the `scripts/run_vicariance_analysis.R`
script, and it has 800 sets. The `bash/run_vicariance_analysis.sh` has a job
array with 1-800 jobs dispatched. Unlike the other analyses above, this analysis
can be run by calling a single command: `sbatch bash/run_vicariance_analysis.sh`.
This runs the 800 jobs, one for each parameter set (controlled by the job array
index).

### Processing vicariance results

The vicariance results are also post-processed, this time to collate the
results into a single table with all parameter sets. The third function
in the `scripts/post_process.R` script

```r
post_process_vicariance_results(
  data_folder_path = file.path("inst", "pre_processed_vicariance_results"),
  output_file_path = file.path("inst", "post_processed_vicariance_results")
)
```

will collate and save the post-processed results and put them in the
`inst/post_processed_vicariance_results` directory. From there they it can be
committed to version control.

## Data Visualisation

Plots can be produced by running the `script/data_vis.R` R script.
