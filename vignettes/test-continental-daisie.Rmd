---
title: "Test Continental DAISIE"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Test Continental DAISIE}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(DAISIEcontinental)
```

The {DAISIE} R package contains maximum likelihood models to infer island biogeographical and macroevolutionary processes on reconstructed phylogenetic data of island communities. A new model in the DAISIE model arsenal is the continental island model. This model adjusts the initial conditions of the DAISIE model and introduces a new parameter (`p`), the probability of initial presence. In other words, the parameter defines what proportion of the island community derives from a vicariance event at the origin of the island compared to colonising the island after it formed. 

The {DAISIEcontinental} R package is a set of tools (functions and scripts) to establish and quantify how accurate the continental DAISIE model for a variety of island scenarios (e.g. different island ages, and community size).

The package has two principal analysis:

* continental inference model estimation performance with simulated data
* profile likelihood of empirical data at fixed values of `p`

The `run_empirical_analysis` (bash and R script) can be used on empirical data when we do not know the try value of `p`, and want to understand the profile of the likelihood. The `run_empirical_analysis` script is used when empirical data is available and we want to determine how the maximised likelihood of the model changes as we fix the probability of initial presence to different values between its bounds (`[0, 1]`). The `run_analysis` (bash and R script) is to determine the precision of the model with a known value of `p`. The `run_analysis` script is used when we want to determine how well the continental model can estimate the probability of initial presence for a given simulated scenario.

This vignette will walk through the application of both of these analyses and show how they can be used to analyse the continental DAISIE model. The package leverages the High Performance Cluster Computer (HPCC) from the University of Groningen to run the analyses in a distributed manner to parallelise computationally demanding analyses.

## Reproducing parameter estimation analysis

### Running the analysis on the HPC

* Clone the repository
* Change the working directory into `/DAISIEcontinental`
* Install the repository by running `sbatch bash/install_DAISIEcontinental.sh`
* Once the installation has finished, run the analysis this requires multiple
steps.

The analysis cannot be submitted using a single command. This is because the 
analysis is run over a parameter space, with each parameter set being run
for several replicates. Each replicate is a single job submitted by job array
(see `bash/run_analysis.sh`).

Each parameter set in the parameter space cannot be submitted by a single 
bash script as it would require two job arrays, one going over the parameter
sets and a second over the replicates. However, this case (which could be set
up by having a master bash script which arrays over the parameter space which
submits multiple lower-level bash scripts each submitting multiple replicates)
does not work in practise as it would write the logs of multiple jobs to the 
same log file. This cannot be worked around as the `#!/bin/bash` bash header
does not accept arguments to name the output log files.

Therefore, what works is to manually submit each parameter set manually by
specifying the parameter set index with an argument to the bash script. There
are 18 parameter sets so the entire parameter space can be run by running the 
following commands (still with `DAISIEcontinental` as the working directory):

* `sbatch bash/run_analysis.sh 1`
* `sbatch bash/run_analysis.sh 2`
* `sbatch bash/run_analysis.sh 3`
* ...
* `sbatch bash/run_analysis.sh 16`
* `sbatch bash/run_analysis.sh 17`
* `sbatch bash/run_analysis.sh 18`

This results in each log and results file having a unique file name and each
job writing to a single file.

The reasoning for having the the jobs so distributed, i.e. each job only runs
a single replicate for three different island ages, is because running more
simulations and model fitting sequentially does not scale well for the number
of replicates. To ensure an adequate sample size of results the model should be
run for each island age and each parameter set several hundred times, to account
for model stochasticity. Simulations that have a large community size (number of
species) take several minutes to run each. If these were to run sequentially
for, say 500 replicates, each with three island ages, totalling 1500 replicates
it would take an exceedingly long time to compute. The setup implemented
reduces this run time by running replicates over separate cores on a cluster.

The results can then be copied to a directory in `\inst` called 
`pre_processed_daisie_results`. If this folder is not already present it can be
created by running `mkdir inst/pre_processed_daisie_results`. The results can
be copied by running `cp -r results/ inst/pre_processed_daisie_results`. These
results can then be pushed to the repository.

### Processing DAISIE results

Before plotting the results, they need to be collated and formatted. This is
done by using the `post_process_daisie_results()` function. The 
`post_process_daisie_results.R` script in `scripts/` can be run either by
opening R and running the code (either copying and pasting into the console or
calling with `source()`), or can be run from the terminal with 
`Rscript scripts/post_process_daisie_results.R` (note in both cases the 
DAISIEcontinental package will need to be load to use the function). This 
function will automatically save the post-processed files in 
`inst/post_processed_daisie_results/`.

The pre-processed results from the cluster jobs can be deleted as there are a 
large number of files and there is no need to have them tracked by version 
control. Therefore, the `inst/pre_processed_daisie_results` directory can be
deleted with `rm -r inst/pre_processed_daisie_results`.

## Reproducing profile likelihood empirical analysis

The empirical analysis uses the same startup steps as the parameter estimation
analysis. If these steps have already been done for the above analysis they do
not need to be repeated.

* Clone the repository
* Change the working directory into `/DAISIEcontinental`
* Install the repository by running `sbatch bash/install_DAISIEcontinental.sh`

The empirical analysis uses the same job setup and submission as the parameter
estimation analysis. The entire empirical analysis also cannot be submitted 
with a single command, and instead each parameter set is submitted individually, 
specifying the param set index as an argument. There are 50 parameter sets,
which includes each taxonomic group and each fixed probability of initial 
presence of species on the continental island.

* `sbatch bash/run_empirical_analysis.sh 1`
* `sbatch bash/run_empirical_analysis.sh 2`
* `sbatch bash/run_empirical_analysis.sh 3`
* ...
* `sbatch bash/run_empirical_analysis.sh 48`
* `sbatch bash/run_empirical_analysis.sh 49`
* `sbatch bash/run_empirical_analysis.sh 50`

### Processing empirical results

The empirical results are also post-processed to collate the results into each
parameter set. The second function in the `scripts/post_process_daisie_results`

```r
post_process_empirical_analysis(
  data_folder_path = file.path("inst", "pre_processed_empirical_results"),
  output_file_path = file.path("inst", "post_processed_empirical_results")
)
```

will create the post-processed results and put them in the
`inst/post_processed_empirical_results` directory. From there they can be
commited to version control, and used to make plots.
